# Libraries used
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from keras.callbacks import History
from zlib import crc32
import re

from helpermodules.memory_handling import PickleHelper

def hashing_and_splitting(adj_close_stocks_dataframe):
    """
    Splits the given DataFrame of adjusted close dataframe into training and testing sets based on checksum hashing.

    Parameters:
        adj_close_stocks_dataframe (pandas.DataFrame): DataFrame containing adjusted close dataframe.

    Returns:
        Tuple[pandas.DataFrame, pandas.DataFrame]: A tuple containing the training and testing DataFrames.
    """
    checksum = np.array([crc32(v) for v in adj_close_stocks_dataframe.index.values])
    test_ratio = 0.2
    test_indices = checksum < test_ratio * 2 ** 32
    return adj_close_stocks_dataframe[~test_indices], adj_close_stocks_dataframe[test_indices]


def xtrain_ytrain(adj_close_stocks_dataframe):
    """
    Splits the DataFrame into training and testing sets, normalizes the data, and prepares it for LSTM model training.

    Parameters:
        adj_close_stocks_dataframe (pandas.DataFrame): DataFrame containing adjusted close dataframe.

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, MinMaxScaler]: A tuple containing training and testing data along with the scaler.
    """
    split_index = int((len(adj_close_stocks_dataframe)) * 0.80)
    train_set = pd.DataFrame(adj_close_stocks_dataframe.iloc[0:split_index])
    test_set = pd.DataFrame(adj_close_stocks_dataframe.iloc[split_index:])

    sc = MinMaxScaler(feature_range=(0, 1))
    sc.fit(train_set)
    training_set_scaled = sc.fit_transform(train_set)
    test_set_scaled = sc.transform(test_set)

    xtrain = []
    ytrain = []
    for i in range(60, training_set_scaled.shape[0]):
        xtrain.append(training_set_scaled[i - 60:i, 0])
        ytrain.append(training_set_scaled[i, 0])
    xtrain, ytrain = np.array(xtrain), np.array(ytrain)
    xtrain = np.reshape(xtrain, (xtrain.shape[0], xtrain.shape[1], 1))

    xtest = []
    ytest = []
    for i in range(20, test_set_scaled.shape[0]):
        xtest.append(test_set_scaled[i - 20:i, 0])
        ytest.append(test_set_scaled[i, 0])
    xtest, ytest = np.array(xtest), np.array(ytest)
    return xtrain, ytrain, xtest, ytest, sc


def lstm_model(xtrain, ytrain):
    """
    Builds and trains an LSTM model using the training data.

    Parameters:
        xtrain (np.ndarray): Input training data.
        ytrain (np.ndarray): Target training data.

    Returns:
        Sequential: Trained LSTM model.
    """
    model = Sequential()
    #1st Layer
    model.add(LSTM(units=50, activation='relu',
                  return_sequences=True, input_shape=(xtrain.shape[1], 1)))
    model.add(Dropout(0.2))#Turn off 20% of the neurons casually
    #2nd Layer
    model.add(LSTM(units=60, activation='relu', return_sequences=True))
    model.add(Dropout(0.3))
    #3rd Layer
    model.add(LSTM(units=80, activation='relu', return_sequences=True))
    model.add(Dropout(0.4))
    #4th Layer
    model.add(LSTM(units=120, activation='relu')) #We don't return (no need to add other layers)
    model.add(Dropout(0.5))

    model.add(Dense(units=1)) # nÂ°of Output
    model.compile(optimizer='adam', loss='mean_squared_error')

    #Train (Parameters to be tried)
    model.fit(xtrain, ytrain, epochs=50, batch_size=32)

    return model
